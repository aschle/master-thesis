\chapter{Approach and Implementation}
\label{ch:approach}
In this chapter the basic work flow is described in detail. The process is mainly driven by an exploratory approach, but follows primarily Farines and Whiteheads~\cite{farine2015constructing} primary steps and key considerations for social network analysis to non-human animal data. The adapted and resulting process is visualized in figure~\ref{fig:process}.

The dataset was first analysed regarding data quality and to form an understanding of the dataset and behaviour of bees in general. Those findings were used to define nodes and infer associations to build the network, respectively derive parameters for the network pipeline. The static and temporal networks are analysed using network science tools and methods. For testing hypothesis the networks are combined with spatial and age information. Each step is explained within the following sections.

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\textwidth]{Figures/process}
	\caption{Steps of the Research Approach}
	\label{fig:process}
\end{figure}


\section{The Dataset}
\label{sec:dataset}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\textwidth]{Figures/setupCams}
	\caption{Camera Setup in 2016: (1) Top View:  vertical hive with two cameras for each side, overlapping in the middle. (2) Front View: left and right camera setup, the red dot indicates the origin $(0,0)$ of the camera image.}
	\label{fig:cams}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{Figures/recording}
	\caption[Recording Season]{Recording Season with maintainance and failures: \emph{Green} indicates recording went without any big interruption; \emph{Yellow} indicates maintainance work or technical failures of one or all cameras. This is calculated using the expected number of files produced by each camera per hour.}
	\label{fig:period}
\end{figure}

The dataset derives from video files, that capture tagged honey bees of one colony in an observation hive.
Each individual of the colony, including about 3000 bees, were tagged with circular 12-bit markers. Four cameras were used to film the hive, the setup of the cameras is illustrated in figure~\ref{fig:cams} and an example of tagged bees is shown in section~\ref{ch:intro}, figure~\ref{fig:markers}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\textwidth]{Figures/tagging_period}
	\caption[Tagging Frequency]{Tagging frequency of bees: The bees were primarily tagged during the week. On average 48 bees were tagged each day, considering only tagging days, the average is about 91 ($\pm50$) bees (median 118).}
	\label{fig:tagging}
\end{figure}

The recording season lastet nine weeks (63 days), around the clock, from 19.07.2016 until 19.09.2016, with some interruptions due to maintainance and technical failures (figure~\ref{fig:period}). The recording resolution of each camera is three frames per second, with 1024 frames per video file. For each frame, bees were detected by using an image analysis pipeline, which is explained in detail in~\cite{wario2015automatic}. The resulting detection data is stored in a binary file format.
A python library called \emph{bb-binary}\footnote{\url{https://github.com/BioroboticsLab/bb_binary}; Last accesed: 2106-02-16, 04:28PM} provides easy access to the binary files. Each file in bb\_binary file format corresponds to a video file of a single camera.
The size of the complete dataset for 2016 is $470$~GB, about $7.5$~GB of binary data per day.

Exactly $3.191$ bees were tagged. The tagging period was 67 days long. The tagging started on 28.06.2016 (22 days before the recording started) and lasted until 02.09.2016 (17 days before the recording ended). The young bees were tagged and then added to the hive, about noon each day. The overall tagging frequency is shown in figure~\ref{fig:tagging}. The hatching day for each bee was documented. Therefore the age of each bee at a certain point in time can be calculated.

\subsection{Structure of the Dataset}
The data is organised in \emph{frame container}, wich corresponds to a video file of a single camera. A frame container holds all \emph{frames} for that specific video.
Each frame has a list of all detected bees.
A \emph{detection} has the following attributes, which are relevant to this project:

\begin{itemize}
\item \textbf{xpos}: $x$ coordinate of bee with respect to the image in pixel
\item \textbf{ypos}: $y$ coordinate of bee with respect to the image in pixel
\item \textbf{radius}: of the circular 12-bit tag
\item \textbf{decodedId}: decoded 12-bit id
\end{itemize}

Besides further information, the frame container specifies the camera, which took the video. A frame is also attributed with a timestamp. The data can be accessed iterating on the frame level, using two timestamps (start and end) for specifying the time interval. The complete data scheme can be found on github\footnote{\url{https://github.com/BioroboticsLab/bb_binary/blob/master/bb_binary/bb_binary_schema.capnp}; Last accessed: 2106-02-16, 04:46PM}. 


\subsection{ID Probabilities and Confidence Level}
\label{subsec:confidence}
With a 12-bit ID, 4096 bees can be tagged.
Each bit of the decodedId is not a $1$ or $0$, but represents a probability between $0$ and $255$. It indicates the reliablility of the image analysis pipeline for that specific bit.  A number closer to $255$ represents a $1$, a number closer to $0$ a $0$.
A \emph{confidence value} can be calculated for each ID. The confidence value of a single bit is its distance to $128$, discretized to a value between $0$ and $1$. The confidence value of an ID is therefore the minimum of all bit confidences.

The amount of data that remains for further processing and its quality  depends heavily on the chosen \emph{confidence level}. Figure~\ref{fig:confidence} shows the relationship between confidence level, the amount of remainig data and the number of unique IDs. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/confVSamount}
        \caption[Remaining Detections]{Remaining Detections depending on the level of confidence.}
        \label{fig:confVSamount}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/confVSids}
        \caption[Unique IDs]{Number of unique IDs depending on the level of confidence.}
        \label{fig:confVSids}
    \end{subfigure}
    \caption[Amount of Remaining Data]{The amount of remaining data decreases with an increasing confidence level. The number of unique IDs behave similar. (Dataset: 26.07.2016, 16:00-16:05)}
    \label{fig:confidence}
\end{figure}

\subsection{Time Series of Bees}
\label{subsec:tracking}

The original dataset (a number of frames containing bee detections), is transformed to binary \emph{time series of bees}, depicted in figure~\ref{fig:structure} (left and middle). A time series of a bee is a sequence of zeros and ones indicating the absence and presence of a bee over a specified time interval. 
As expected the confidence level has an effect on the resulting time series of a bee. A high confidence leads to more gaps in the series and also to longer gaps.

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0\textwidth]{Figures/structure}
	\caption[Structure of Dataset]{\textbf{Left:} original dataset - bb\_binary data containing frames and detections; \textbf{Middle:} transformation to time series - zero indicating absence of the bee, one indicating presence of the bee; \textbf{Right:} transformation to bee pairs - zero indicating eighter one or both bees are not present at the same time or not close to each other, one indicating bees are present at the same time and nearby.}
	\label{fig:structure}
\end{figure}


\subsection{Data Quality}
\label{subsec:quality}

The quality of the data could be indirectly checked using the age information of bees. On 26.07.2016, about half of the bee tags ($2014$ of $4096$) were assigned to a bee. This day was chosen to determine the effects of the confidence level on data quality.
At first, for each detection the age of that bee was calculated, a negative age was counted as a wrong detection. I assumed that the number of wrong detections indicated by negative age also occured in the positive half, but remained unseen, therefore I doubled the error. Secondly, the number of wrong unique IDs is also determinde using the age test. Figure~\ref{fig:quality} shows that even though the number of wrong detections
decreases steadily with an increasing confidence level, but the number of wrong IDs only starts to decrease with a very high level.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/confVSdetquality}
        \caption[Wrong Detections]{Wrong detections in relation to the level of confidence.}
        \label{fig:confVSdetquality}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/confVSidsquality}
        \caption[Unique IDs]{Number of wrong IDs depending on the level of confidence.}
        \label{fig:confVSidsquality}
    \end{subfigure}
    \caption[Data Quality]{The number of wrong detections decreases with an increasing level of confidence. In contrast, the number of false IDS becomes noticeably less only with a very high confidence level. (Dataset: 26.07.2016, 16:00-16:05)}
    \label{fig:quality}
\end{figure}

Even with an confidence level of $100\%$, $30.2\%$ of the unique IDs are wrong (have a negative age), corresponding to only $2.5\%$ of detections. Therefore wrong IDs need to be filtered out anyway (independent of the confidence value), to obtain a more reliable dataset. A good indicator is the detection frequency of IDs. IDs with a negative age are on average less detected than IDs with a positive age.


\subsection{Implications}
For further analysis I use the days 30.07.2016, 31.07.2016, 01.08.2016 and 02.08.2016 with a confidence level $95\%$ [TODO: evaluate confidence level]. This period is chosen, because it is a contiguous time span close to the recording start.
Because of the high fragmentation level, the inferring of edges (bees who are close to each other at the same time), should be not that strict, or at least variable. This has to be taken into acoount, when looking at spatialy close bees.


\section{Inferring Networks}

The following part describes the pipeline for generating spatial proximity networks out of honey bee tracking data. A node in the network is a bee. They are distinguished by IDs. Only bees are in the network who interact with other bees, that means they have at least one edge and a degree of one.

Two bees are associated (spatially close to each other), if their distance is minor to a \emph{maximum distance}. As everything is very close in a bee hive this value is hard to choose. Only this criteria is very week, meaning having a resolution of three frames per seconds results in interactions which could only last for $0.33$ seconds. So an additional parameter the \emph{minimum contact duration} is introduced, it is the minimum time they have to spend at least nearby to be called associated.

Taking the fragmentation of tracks into account, it is obvious that two bees could be nearby but not at the very same time, but slightly shifted. So the minimum contact duration would be too errow prone. To overcome this issue one could correct the bee tracks, by filling gaps of varius sizes and interpolating the position of that bee accordingly. This is rather time consuming for this amount of tracking data (TODO: naja so doll auch nicht, scipy.ndimage.morphology.binary\_dilation) and also considering, that the tracking data is going to be improved in the future, then manipulating the raw data seems senseless. I rather perform a gap filling (maybe similar to binary dilation) on the time series of pairs, but not on the bee tracks, because this is independent of the input data.

To evaluate this method I compared the hemming distance of resulting bee pairs for (1) Correcting Bee Tracks (fill gaps size one), then extract pairs, (2) Bee tracks without correction, then extract pairs and (3) Bee Tracks without correction, but corrected extracted pairs (fill gaps size one). [TODO: Wie soll das Ergenis hier hin?]

\subsection{Network Pipeline}

The network pipeline takes as input a path to an bb-binary repository and outputs a graph in graphML file format. All cameras are included in the pipeline. The pipeline takes the following parameters:

\begin{itemize}
\item path to data
\item id confidence in the range 0 to 1
\item gap size in frames - size of gap, which is used to corret bee pair time series
\item maximum distance in px - maximum distance of bees (spatial proximity)
\item minimum contact duration in frames - how many frames they need to spend nearby
\item start timestamp - start of network slice
\item window size in minutes - size of time window for aggregating the network
\item number of used CPUs for parallelization
\end{itemize}

The pipeline is parallelized on frame level, that means, each process gets a portion of the data and extracts interactions, the main process, adds everything up and creates the network. The steps are the following:

\begin{enumerate}
\item For each camera: filter detections by ID confidence
\item Simple stitching of the two sides of the hive.
\item Extract close pairs.
\item Generate time series for each pair.
\item Correct pair time series.
\item Extract interactions/contacts from time series.
\end{enumerate}


\subsection{Edge weight and Thresholding}
Edges are attributed with three parameters. The first one is the frequency of contacts, so how often they share a close position. The second parameter is the total duration of contact, how many time frames in total they spend close by.
 
TODO. document what threshold for edges was used.

\subsection{Pipeline Parameters}
For performing the following network analysis, I chose the parameters as follows:

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/radius}
		\caption[Contact Radius]{Contact Radius}
		\label{fig:radius}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
	    \includegraphics[width=\textwidth]{Figures/sizeTagBee}
        \caption[Bee and Tag Size]{Bee and Tag Size}
        \label{fig:size}
    \end{subfigure}
    \caption{Distance Between Bees: The average bee length is 212px, according to this, the maximal distance between bees is chosen.}
    \label{fig:contactRadius}
\end{figure}

\begin{description}
\item[Confidence] As explained in section\ref{subsec:confidence}, the confidence is set to $95\%$.

\item[Maximal Distance] I chose the length of a bee body $212$px ($\pm 16$px), according to \textcite{baracchi2014socio}, as the maximal distance between two bees (see figure~\ref{fig:radius}). The average bee length was determinded by manually measuring the length of all bees ($n=337$) in four images (one for each camera, 21.07.2016, 03:00PM) using the tool ImageJ\footnote{\url{http://imagej.net/Welcome}; Last accessed: 22.02.2016}.

\item[Minimal Contact Duration] This is set to 3 frames (one second), to exclude errors. TODO: I do not know why actually and how to justify this. \textcite{mersch2013tracking} excluded interactions below one second as well. I would rather look at the average speed of moving bees, to exclude passing bees, but then shorter interactions are excluded. Looking at the length of 1-chains of the pair time series (after filling the gaps), the mode=1, median=2 and mean=4. Three frames corresponds to 57\% of all 1-chains, seem to be reasonable. (4 = 66\%) (same dataset as in gap size)

\item[Gap Size] The gap size is set to two frames. This value corresponds to the median of gap length of bee tracks (mode=1, mean=14) and median of gap length in pair time series (mode=1, mean=27). This is tested for a 10 minute dataset for camera 1 (because it  had the most detections).
\end{description}


\section{Static and Temporal Analysis}
dataset used here 1 day and one hour of that day\\

\subsection{Static Network Analysis}
The following network properties were analysed for a static day and hour network.\\
TODO: list of properties. (similar to what others have done)

\subsection{Temporal Analysis}
consecutive day networks\\
one day split up in hours\\

\section{Attributed Data and Hypothesis Testing}
Hypothesis\\
(1) Communities reflect groups of bees working in different areas of the hive and\\
(2) Communities reflect different age groups\\

The data which was used to test the hypothesis (1) is saved in a sqlite database for faster access, because using bb\_binary (parsing the data over and over again) was to slow. For testing if lists of positions (spatial ditribution) are different the test XY was used [TODO: what to use here]

For hypothesis (2) the data is stored as a csv file of birth dates of each bee. For testing if age goups are different the Kolmogorov Smirnov Test was used.

\section{Implementation, Runtime and Complexity}
For implementing the network pipeline python, with pandas and numpy, are used, because the bb\_binary library, for accessing the tracking, data is only available in python. The networks, in graphML format, are created using the python library \emph{NetworkX}\footnote{\url{https://networkx.github.io/} ; Last accessed: 2017-02-17, 08:07PM} in version 1.11.
iGraph for community detection\\
some bash scrips for generating multiple networks\\

bottleneck is reading bb\_binary data into pandas dataframes\\
using multithreading for distribution on frame level (a process gets X frames for processing)\\

maybe some table with how long nees what with how many cores (hom much RAM and so on)\\